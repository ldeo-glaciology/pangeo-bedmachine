{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My attempts to save datasets into zarrs in the google bucket. \n",
    "I am trying it with bedmachine and with PISM output.\n",
    "\n",
    "One problem was with the bedmachine dataset. I sorted that and then managed to get a small dataset (initially loaded from a netcdf) into and back out of the google bucket. Now the issue that I am running out of memory and the server crashes. \n",
    "\n",
    "Next step is to try to get it running on the cluster, but .to_zarr does not run on the cluster as it stands. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.array as da\n",
    "import dask.delayed\n",
    "from dask.distributed import Client\n",
    "import dask_gateway\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "xr.set_options(display_style=\"html\")\n",
    "import fsspec\n",
    "import gcsfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://storage.googleapis.com/ldeo-glaciology/bedmachine/BedMachineAntarctica_2019-11-05_v01.nc'  \n",
    "with  fsspec.open(url, mode='rb')  as openfile:  \n",
    "    bm = xr.open_dataset(openfile)  \n",
    "\n",
    "# remove the variable mapping because it was causing an error in the write to zarr\n",
    "bm.attrs = {**bm.attrs, **bm.mapping.attrs} # but keep the information in the attributes of the whole dataset. \n",
    "bm = bm.drop('mapping')   # remove the variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take small subset of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.76"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_small = bm.isel(x=slice(0,20), y = slice(0, 20))  # create a very small version of the dataset for testing the upload and download\n",
    "bm_small.nbytes/1e3   #  it is only 10 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This can be written to a zarr directoryi in our bucket no problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_small_mapper = fsspec.get_mapper('gs://ldeo-glaciology/temp/bm_small4.zarr', mode='ab',\n",
    "                            token='../secrets/ldeo-glaciology-bc97b12df06b.json')  # get a mapper object using the token stored in the ooi environment\n",
    "bm_small.to_zarr(bm_small_mapper, mode='w');   # write the dataset to zarr in the google basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bm_small_mapper = fsspec.get_mapper('gs://ldeo-glaciology/temp/bm_small4.zarr') # This also works - just to make sure we dont need the token to access\n",
    "bm_small_reloaded = xr.open_zarr(bm_small_mapper) # reload the dataset using the same mapper as before\n",
    "bm_small_reloaded.identical(bm_small)    # check that what we get back is the same as what we tried to load up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85880a04b034e36b5a8cbca580c8b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>GatewayCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n<style scoped>\\n    â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the dask-gateway version\n",
    "dask_gateway.__version__\n",
    "# show the default dask-gateway settings\n",
    "dask.config.config['gateway']\n",
    "#default gateway call\n",
    "gateway = dask_gateway.Gateway()\n",
    "# default new_cluster call\n",
    "cluster = gateway.new_cluster()\n",
    "#gateway = Gateway()\n",
    "gateway.list_clusters()\n",
    "# the dashboard_link property will show the link that can be pasted into the Dask labextension\n",
    "cluster.dashboard_link\n",
    "# scale cluster to 8 workers using the scale() method\n",
    "cluster.scale(8)\n",
    "# connect a client\n",
    "# the distributed client is used for running parallel tasks with Dask\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell is according to https://gist.github.com/rabernat/4cc2eca3868abda7ddf89ed10f8007fb, how you get .to_zarr to run on the cluster. Currently it throws an error because gcfs_auth.tokens comes back an empty dict, when I think it should have some entries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "('ldeo-glaciology', 'full_control')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-fb9f06280695>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgcfs_auth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcsfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGCSFileSystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ldeo-glaciology'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../secrets/ldeo-glaciology-bc97b12df06b.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcfs_auth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ldeo-glaciology'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'full_control'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgcfs_w_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcsfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGCSFileSystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ldeo-glaciology'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgcsmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcsfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGCSMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gs://ldeo-glaciology/temp/bm_small4.zarr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgcs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgcfs_w_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_zarr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgcsmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ('ldeo-glaciology', 'full_control')"
     ]
    }
   ],
   "source": [
    "gcfs_auth = gcsfs.GCSFileSystem(project='ldeo-glaciology', token='../secrets/ldeo-glaciology-bc97b12df06b.json')\n",
    "token = gcfs_auth.tokens[('ldeo-glaciology', 'full_control')]\n",
    "gcfs_w_token = gcsfs.GCSFileSystem(project='ldeo-glaciology', token=token)\n",
    "gcsmap = gcsfs.GCSMap('gs://ldeo-glaciology/temp/bm_small4.zarr', gcs=gcfs_w_token)\n",
    "ds.to_zarr(gcsmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to load the full dataset into zarr fails on the largest version of ooi.pangeo.io, but on the larger https://us-central1-b.gcp.pangeo.io/ it works. \n",
    "\n",
    "## In neither case does it appear to use the cluster to do the computation, meaning that the whole dataset has to be loaded to the notebook server, causing it to crashs unless we use a large machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_mapper = fsspec.get_mapper('gs://ldeo-glaciology/bedmachine/bm.zarr', mode='ab',\n",
    "                            token='../secrets/ldeo-glaciology-bc97b12df06b.json')\n",
    "bm.to_zarr(bm_mapper, mode='w');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_reloaded = xr.open_zarr(bm_mapper)  \n",
    "bm_reloaded.identical(bm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "asyncio.exceptions.CancelledError\n"
     ]
    }
   ],
   "source": [
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to get .to_zarr to use the cluster?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
